{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load up pretrained word embeddings so we can do word index lookups.  In production this should just be a pickled dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../data/GoogleNews-vectors-negative300.bin\"\n",
    "word_vectors = KeyedVectors.load_word2vec_format(path, binary=True)\n",
    "word2ind = {k: v.index for k,v in word_vectors.vocab.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create lookups for words to category number and vice versa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'Literature',\n",
      " 1: 'Social Science',\n",
      " 2: 'History',\n",
      " 3: 'Science',\n",
      " 4: 'Fine Arts',\n",
      " 5: 'Trash',\n",
      " 6: 'Religion',\n",
      " 7: 'Philosophy',\n",
      " 8: 'Geography',\n",
      " 9: 'Mythology',\n",
      " 10: 'Current Events'}\n"
     ]
    }
   ],
   "source": [
    "category_lookup = {'Literature': 0, 'Social Science': 1, 'History': 2, 'Science': 3, 'Fine Arts': 4, 'Trash': 5, 'Religion': 6, 'Philosophy': 7, 'Geography': 8, 'Mythology': 9, 'Current Events': 10}\n",
    "index2category = {v:k for k, v in category_lookup.items()}\n",
    "pprint(index2category)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to load the saved model, you need the class loaded in memory.  This should be factored out to a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DanModel(nn.Module):\n",
    "\n",
    "    def __init__(self, n_classes, n_hidden_units=50, nn_dropout=.5):\n",
    "        super(DanModel, self).__init__()\n",
    "        self.n_classes = n_classes\n",
    "        self.n_hidden_units = n_hidden_units\n",
    "        self.nn_dropout = nn_dropout\n",
    "        \n",
    "        self.vocab_size, self.emb_dim = word_vectors.vectors.shape\n",
    "        self.embeddings = nn.Embedding(self.vocab_size, self.emb_dim, padding_idx=0)\n",
    "        self.embeddings.weight.data.copy_(torch.from_numpy(word_vectors.vectors))\n",
    "        self.embeddings.weight.requires_grad = False\n",
    "\n",
    "        self.linear1 = nn.Linear(self.emb_dim, n_hidden_units)\n",
    "        self.linear2 = nn.Linear(n_hidden_units, n_classes)\n",
    "        self.classifier = nn.Sequential(\n",
    "            self.linear1,\n",
    "            nn.ReLU(),\n",
    "            self.linear2)\n",
    "        self.softmax = nn.Softmax()\n",
    "\n",
    "    def forward(self, input_text, text_len):\n",
    "        \"\"\"\n",
    "        Model forward pass\n",
    "\n",
    "        Keyword arguments:\n",
    "        input_text : vectorized question text\n",
    "        text_len : batch * 1, text length for each question\n",
    "        is_prob: if True, output the softmax of last layer\n",
    "\n",
    "        \"\"\"\n",
    "        # get word embeddings\n",
    "        text_embed = self.embeddings(input_text)\n",
    "\n",
    "        # calculate the mean embeddings\n",
    "        encoded = text_embed.sum(1)\n",
    "        encoded /= text_len.view(text_embed.size(0), -1)\n",
    "\n",
    "        # run data through the classifier\n",
    "        logits = self.classifier(encoded)\n",
    "\n",
    "        return self.softmax(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load pretrained DAN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(\"../data/topic-dan-83.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filename, ignore_ratio=0, rebalance=False):\n",
    "    data = list()\n",
    "    with open(filename) as json_data:\n",
    "        questions = json.load(json_data)[\"questions\"]\n",
    "        questions = questions[:int(len(questions) * (1- ignore_ratio))]\n",
    "        \n",
    "        for q in questions:\n",
    "            q_text = q['text'].split()\n",
    "            label = category_lookup[q['category']]\n",
    "            data.append((q_text, label))\n",
    "    return data\n",
    "\n",
    "test_file = \"../data/qanta.test.2018.04.18.json\"\n",
    "test_exs = load_data(test_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# View a single test example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: \n",
      "One work by this author uses printing, gunpowder, and the compass as symbols of personal ambition, national ambition, and the ambition of the human race to extend its grasp. This thinker described three forms of false learning as \"delicate\", \"contentious\", and \"fantastical\" in categorizing the \"distempers\" that impede academic progress. This thinker imagined a utopian university called Salomon's House, and he likened received systems of philosophy to stage plays that misrepresent the world, and thus labeled them \"idols of the theatre\". This author of The New Atlantis established the doctrine of inductive, empirical methodology. For 10 points, name this 17th-century English philosopher who wrote Novum Organum and spearheaded the Scientific Revolution.\n",
      "\n",
      "Category No: 7\n"
     ]
    }
   ],
   "source": [
    "sent, answer = test_exs[0]\n",
    "print(\"Input: \\n{}\".format(\" \".join(sent)))\n",
    "print(\"\\nCategory No: {}\".format(answer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test single prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_category(sent):\n",
    "    # create Tensor of word indexes\n",
    "    vec_text = [0] * len(sent)\n",
    "    for idx, token in enumerate(sent):\n",
    "        if token in word2ind:\n",
    "            vec_text[idx] = word2ind[token]\n",
    "    vec = torch.LongTensor([vec_text])\n",
    "\n",
    "    # run word vector through model\n",
    "    logits = model(vec, torch.Tensor([[1]]))\n",
    "    \n",
    "    # find most likely answer from logits\n",
    "    _, answers = logits.topk(1)\n",
    "    answer = answers.tolist()[0][0]\n",
    "    \n",
    "    # return category name and index\n",
    "    return index2category[answer], answer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: 3\n",
      "Prediction: ('Science', 3)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "index = 244\n",
    "sent, answer = test_exs[index]\n",
    "print(f\"Answer: {answer}\")\n",
    "print(\"Prediction: {}\\n\".format(predict_category(sent)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cmsc723",
   "language": "python",
   "name": "cmsc723"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
