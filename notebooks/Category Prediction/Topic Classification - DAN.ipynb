{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DAN Classification\n",
    "\n",
    "Builds/trains a Deep Averaging Network in order to classify text into a Quiz Bowl category.  Uses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "import random\n",
    "from pprint import pprint\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "from gensim.models import word2vec\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load prebuilt word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../../data/GoogleNews-vectors-negative300.bin\"\n",
    "word_vectors = KeyedVectors.load_word2vec_format(path, binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create DAN model and related code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model = \"topic-dan.pt\"\n",
    "grad_clipping = 5\n",
    "checkpoint = 500\n",
    "\n",
    "class DanModel(nn.Module):\n",
    "\n",
    "    def __init__(self, n_classes, n_hidden_units=50, nn_dropout=.5):\n",
    "        super(DanModel, self).__init__()\n",
    "        self.n_classes = n_classes\n",
    "        self.n_hidden_units = n_hidden_units\n",
    "        self.nn_dropout = nn_dropout\n",
    "        \n",
    "        self.vocab_size, self.emb_dim = word_vectors.vectors.shape\n",
    "        self.embeddings = nn.Embedding(self.vocab_size, self.emb_dim, padding_idx=0)\n",
    "        self.embeddings.weight.data.copy_(torch.from_numpy(word_vectors.vectors))\n",
    "        self.embeddings.weight.requires_grad = False\n",
    "\n",
    "        self.linear1 = nn.Linear(self.emb_dim, n_hidden_units)\n",
    "        self.linear2 = nn.Linear(n_hidden_units, n_classes)\n",
    "        self.classifier = nn.Sequential(\n",
    "            self.linear1,\n",
    "            nn.ReLU(),\n",
    "            self.linear2)\n",
    "        self.softmax = nn.Softmax()\n",
    "\n",
    "    def forward(self, input_text, text_len):\n",
    "        \"\"\"\n",
    "        Model forward pass\n",
    "\n",
    "        Keyword arguments:\n",
    "        input_text : vectorized question text\n",
    "        text_len : batch * 1, text length for each question\n",
    "        is_prob: if True, output the softmax of last layer\n",
    "\n",
    "        \"\"\"\n",
    "        # get word embeddings\n",
    "        text_embed = self.embeddings(input_text)\n",
    "\n",
    "        # calculate the mean embeddings\n",
    "        encoded = text_embed.sum(1)\n",
    "        encoded /= text_len.view(text_embed.size(0), -1)\n",
    "\n",
    "        # run data through the classifier\n",
    "        logits = self.classifier(encoded)\n",
    "\n",
    "        return self.softmax(logits)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_lookup = {'Literature': 0, 'Social Science': 1, 'History': 2, 'Science': 3, 'Fine Arts': 4, 'Trash': 5, 'Religion': 6, 'Philosophy': 7, 'Geography': 8, 'Mythology': 9, 'Current Events': 10}\n",
    "\n",
    "def load_data(filename, ignore_ratio=0, rebalance=False):\n",
    "    data = list()\n",
    "    with open(filename) as json_data:\n",
    "        questions = json.load(json_data)[\"questions\"]\n",
    "        questions = questions[:int(len(questions) * (1- ignore_ratio))]\n",
    "        \n",
    "        for q in questions:\n",
    "            q_text = q['text'].split()\n",
    "            label = category_lookup[q['category']]\n",
    "            data.append((q_text, label))\n",
    "    return data\n",
    "\n",
    "class Question_Dataset(Dataset):\n",
    "    \"\"\"\n",
    "    Pytorch data class for question classfication data\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, examples, vobab):\n",
    "        self.examples = examples\n",
    "        self.word2ind = word2ind\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return vectorize(self.examples[index], self.word2ind)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "\n",
    "def vectorize(ex, word2ind):\n",
    "    \"\"\"\n",
    "    vectorize a single example based on the word2ind dict.\n",
    "\n",
    "    Keyword arguments:\n",
    "    exs: list of input questions-type pairs\n",
    "    ex: tokenized question sentence (list)\n",
    "    label: type of question sentence\n",
    "\n",
    "    Output:  vectorized sentence(python list) and label(int)\n",
    "    e.g. ['text', 'test', 'is', 'fun'] -> [0, 2, 3, 4]\n",
    "    \"\"\"\n",
    "    question_text, question_label = ex\n",
    "    vec_text = [0] * len(question_text)\n",
    "\n",
    "    for idx, token in enumerate(question_text):\n",
    "#         vec_text[idx] = word2ind['<unk>']\n",
    "        if token in word2ind:\n",
    "            vec_text[idx] = word2ind[token]\n",
    "\n",
    "    return vec_text, question_label\n",
    "\n",
    "\n",
    "def batchify(batch):\n",
    "    \"\"\"\n",
    "    Gather a batch of individual examples into one batch,\n",
    "    which includes the question text, question length and labels\n",
    "\n",
    "    Keyword arguments:\n",
    "    batch: list of outputs from vectorize function\n",
    "    \"\"\"\n",
    "\n",
    "    question_len = list()\n",
    "    label_list = list()\n",
    "    for ex in batch:\n",
    "        question_len.append(len(ex[0]))\n",
    "        label_list.append(ex[1])\n",
    "    target_labels = torch.LongTensor(label_list)\n",
    "    x1 = torch.LongTensor(len(question_len), max(question_len)).zero_()\n",
    "    for i in range(len(question_len)):\n",
    "        question_text = batch[i][0]\n",
    "        vec = torch.LongTensor(question_text)\n",
    "        x1[i, :len(question_text)].copy_(vec)\n",
    "    q_batch = {'text': x1, 'len': torch.FloatTensor(question_len), 'labels': target_labels}\n",
    "    return q_batch        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_data_loader, dev_data_loader, accuracy, device):\n",
    "    \"\"\"\n",
    "    Train the current model\n",
    "\n",
    "    Keyword arguments:\n",
    "    model: model to be trained\n",
    "    train_data_loader: pytorch build-in data loader output for training examples\n",
    "    dev_data_loader: pytorch build-in data loader output for dev examples\n",
    "    accuracy: previous best accuracy\n",
    "    device: cpu of gpu\n",
    "    \"\"\"\n",
    "\n",
    "    model.train()\n",
    "    optimizer = torch.optim.Adamax(model.parameters())\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    print_loss_total = 0\n",
    "    epoch_loss_total = 0\n",
    "    start = time.time()\n",
    "\n",
    "    for idx, batch in enumerate(train_data_loader):\n",
    "        question_text = batch['text'].to(device)\n",
    "        question_len = batch['len']\n",
    "        labels = batch['labels']\n",
    "\n",
    "        output = model(question_text, question_len)\n",
    "        loss = criterion(output, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        clip_grad_norm_(model.parameters(), grad_clipping)\n",
    "        print_loss_total += loss.data.numpy()\n",
    "        epoch_loss_total += loss.data.numpy()\n",
    "\n",
    "        if idx % checkpoint == 0 and idx > 0:\n",
    "            print_loss_avg = print_loss_total / checkpoint\n",
    "\n",
    "            print('number of steps: %d, loss: %.5f time: %.5f' % (idx, print_loss_avg, time.time()- start))\n",
    "            print_loss_total = 0\n",
    "            curr_accuracy = evaluate(dev_data_loader, model, device)\n",
    "            if accuracy < curr_accuracy:\n",
    "                torch.save(model, save_model)\n",
    "                accuracy = curr_accuracy\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "def evaluate(data_loader, model, device):\n",
    "    \"\"\"\n",
    "    evaluate the current model, get the accuracy for dev/test set\n",
    "\n",
    "    Keyword arguments:\n",
    "    data_loader: pytorch build-in data loader output\n",
    "    model: model to be evaluated\n",
    "    device: cpu of gpu\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    num_examples = 0\n",
    "    error = 0\n",
    "    for idx, batch in enumerate(data_loader):\n",
    "        question_text = batch['text'].to(device)\n",
    "        question_len = batch['len']\n",
    "        labels = batch['labels']\n",
    "\n",
    "        logits = model(question_text, question_len)\n",
    "\n",
    "        top_n, top_i = logits.topk(1)\n",
    "        num_examples += question_text.size(0)\n",
    "        error += torch.nonzero(top_i.squeeze() - torch.LongTensor(labels)).size(0)\n",
    "\n",
    "    accuracy = 1 - error / num_examples\n",
    "    print('accuracy', accuracy)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load JSON files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 13052,\n",
      " 1: 3875,\n",
      " 2: 12979,\n",
      " 3: 11443,\n",
      " 4: 6752,\n",
      " 5: 2123,\n",
      " 6: 1381,\n",
      " 7: 1524,\n",
      " 8: 1435,\n",
      " 9: 1899}\n"
     ]
    }
   ],
   "source": [
    "### Load data\n",
    "train_file = \"../../data/qanta.train.2018.04.18.json\"\n",
    "dev_file = \"../../data/qanta.dev.2018.04.18.json\"\n",
    "test_file = \"../../data/qanta.test.2018.04.18.json\"\n",
    "\n",
    "train_exs = load_data(train_file, .5)\n",
    "dev_exs = load_data(dev_file)\n",
    "test_exs = load_data(test_file)\n",
    "\n",
    "word2ind = {k: v.index for k,v in word_vectors.vocab.items()}\n",
    "\n",
    "pprint(FreqDist([t[1] for t in train_exs]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rebalancing Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 1500,\n",
      " 1: 1500,\n",
      " 2: 1500,\n",
      " 3: 1500,\n",
      " 4: 1500,\n",
      " 5: 1500,\n",
      " 6: 1381,\n",
      " 7: 1500,\n",
      " 8: 1435,\n",
      " 9: 1500}\n"
     ]
    }
   ],
   "source": [
    "def rebalance_with_oversample(exs):\n",
    "    report = FreqDist([t[1] for t in exs])\n",
    "    max_instances = report[report.max()]\n",
    "    data = exs.copy()\n",
    "    balanced_data = []\n",
    "\n",
    "    for k, v in report.items():\n",
    "        multiplier = int(max_instances / v) - 1\n",
    "        filtered = list(filter(lambda item: item[1] == k, exs))\n",
    "        for _ in range(multiplier):\n",
    "            data.extend(filtered)\n",
    "\n",
    "    random.shuffle(data)\n",
    "    return data\n",
    "# train_exs = rebalance_with_oversample(train_exs)\n",
    "\n",
    "\n",
    "def rebalance_with_undersampling(exs, limit=1500):\n",
    "    data = []\n",
    "    report = FreqDist([t[1] for t in exs])\n",
    "    for k, v in report.items():\n",
    "        data.extend(list(filter(lambda item: item[1] == k, exs))[:limit])\n",
    "    random.shuffle(data)\n",
    "    return data\n",
    "\n",
    "train_exs = rebalance_with_undersampling(train_exs)\n",
    "pprint(FreqDist([t[1] for t in train_exs]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Train and Dev data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "\n",
    "# Load batchifed datasets for training (train/dev)\n",
    "train_dataset = Question_Dataset(train_exs, word2ind)\n",
    "train_sampler = torch.utils.data.sampler.RandomSampler(train_dataset)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size,\n",
    "    sampler=train_sampler, num_workers=0, collate_fn=batchify)\n",
    "\n",
    "\n",
    "dev_dataset = Question_Dataset(dev_exs, word2ind)\n",
    "dev_sampler = torch.utils.data.sampler.SequentialSampler(dev_dataset)\n",
    "dev_loader = torch.utils.data.DataLoader(dev_dataset, batch_size=batch_size,\n",
    "    sampler=dev_sampler, num_workers=0, collate_fn=batchify)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create model instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DanModel(\n",
      "  (embeddings): Embedding(3000000, 300, padding_idx=0)\n",
      "  (linear1): Linear(in_features=300, out_features=50, bias=True)\n",
      "  (linear2): Linear(in_features=50, out_features=11, bias=True)\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=300, out_features=50, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=50, out_features=11, bias=True)\n",
      "  )\n",
      "  (softmax): Softmax()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "device = \"cpu\"\n",
    "\n",
    "model = DanModel(11)\n",
    "model.to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start epoch 0\n",
      "number of steps: 500, loss: 2.32067 time: 3.40496\n",
      "accuracy 0.3393501805054152\n",
      "start epoch 1\n",
      "number of steps: 500, loss: 2.04337 time: 2.98606\n",
      "accuracy 0.7856498194945849\n",
      "start epoch 2\n",
      "number of steps: 500, loss: 1.93721 time: 1.92715\n",
      "accuracy 0.8149819494584838\n",
      "start epoch 3\n",
      "number of steps: 500, loss: 1.89062 time: 2.13749\n",
      "accuracy 0.8249097472924187\n",
      "start epoch 4\n",
      "number of steps: 500, loss: 1.84977 time: 2.70411\n",
      "accuracy 0.8226534296028881\n",
      "start epoch 5\n",
      "number of steps: 500, loss: 1.81345 time: 1.82282\n",
      "accuracy 0.8240072202166064\n",
      "start epoch 6\n",
      "number of steps: 500, loss: 1.79307 time: 1.84553\n",
      "accuracy 0.8366425992779783\n",
      "start epoch 7\n",
      "number of steps: 500, loss: 1.78043 time: 1.92983\n",
      "accuracy 0.8267148014440433\n",
      "start epoch 8\n",
      "number of steps: 500, loss: 1.77542 time: 1.86060\n",
      "accuracy 0.8280685920577617\n",
      "start epoch 9\n",
      "number of steps: 500, loss: 1.76258 time: 1.99130\n",
      "accuracy 0.8325812274368232\n",
      "\n",
      "start testing:\n",
      "\n",
      "accuracy 0.8306530214424951\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8306530214424951"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Start Training\n",
    "checkpoint = 500\n",
    "num_epochs = 10\n",
    "accuracy = 0\n",
    "\n",
    "# Create testing dataloader\n",
    "test_dataset = Question_Dataset(test_exs, word2ind)\n",
    "test_sampler = torch.utils.data.sampler.SequentialSampler(test_dataset)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size,\n",
    "    sampler=test_sampler, num_workers=0, collate_fn=batchify)\n",
    "\n",
    "# Train / Fit\n",
    "for epoch in range(num_epochs):\n",
    "    print('start epoch %d' % epoch)\n",
    "    accuracy = train(model, train_loader, dev_loader, accuracy, device)\n",
    "\n",
    "# Test\n",
    "print('\\nstart testing:\\n')\n",
    "evaluate(test_loader, model, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cmsc723",
   "language": "python",
   "name": "cmsc723"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
