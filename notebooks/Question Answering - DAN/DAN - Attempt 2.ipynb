{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DAN Classification\n",
    "\n",
    "Builds/trains a Deep Averaging Network for question answering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "import random\n",
    "from pprint import pprint\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "from gensim.models import word2vec\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load prebuilt word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../data/GoogleNews-vectors-negative300.bin\"\n",
    "word_vectors = KeyedVectors.load_word2vec_format(path, binary=True)\n",
    "word2ind = {k: v.index for k,v in word_vectors.vocab.items()}\n",
    "ind2word = {v:k for k,v in word2ind.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load JSON files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset lengths: (112927, 2216, 4104)\n"
     ]
    }
   ],
   "source": [
    "def load_data(filename, ignore_ratio=0, rebalance=False):\n",
    "    data = list()\n",
    "    with open(filename) as json_data:\n",
    "        questions = json.load(json_data)[\"questions\"]\n",
    "        questions = questions[:int(len(questions) * (1- ignore_ratio))]\n",
    "        \n",
    "        for q in questions:\n",
    "            q_text = q['text'].split()\n",
    "            label = q['page']\n",
    "            data.append((q_text, label))\n",
    "    return data\n",
    "\n",
    "\n",
    "train_file = \"../data/qanta.train.2018.04.18.json\"\n",
    "dev_file = \"../data/qanta.dev.2018.04.18.json\"\n",
    "test_file = \"../data/qanta.test.2018.04.18.json\"\n",
    "\n",
    "train_exs = load_data(train_file)\n",
    "dev_exs = load_data(dev_file)\n",
    "test_exs = load_data(test_file)\n",
    "\n",
    "print(\"dataset lengths: {}\".format((len(train_exs), len(dev_exs), len(test_exs), )))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create answer string dict lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "duplicate answers found in dataset: 92370\n",
      "known answers: 26877\n"
     ]
    }
   ],
   "source": [
    "ans2idx = {}\n",
    "dupes = 0\n",
    "\n",
    "for q in train_exs + dev_exs + test_exs:\n",
    "    if q[1] not in ans2idx:\n",
    "        ans2idx[q[1]] = len(ans2idx.keys())\n",
    "    else:\n",
    "        dupes += 1\n",
    "\n",
    "idx2ans = {v:k for k,v in ans2idx.items()}\n",
    "print(f\"duplicate answers found in dataset: {dupes}\")\n",
    "print(f\"known answers: {len(ans2idx.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create DAN model and related code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model = \"qa-dan.pt\"\n",
    "grad_clipping = 5\n",
    "checkpoint = 50\n",
    "\n",
    "class DanModel(nn.Module):\n",
    "\n",
    "    def __init__(self, n_classes, n_hidden_units=50, nn_dropout=.5):\n",
    "        super(DanModel, self).__init__()\n",
    "        self.n_classes = n_classes\n",
    "        self.n_hidden_units = n_hidden_units\n",
    "        self.nn_dropout = nn_dropout\n",
    "        \n",
    "        self.vocab_size, self.emb_dim = word_vectors.vectors.shape\n",
    "        self.embeddings = nn.Embedding(self.vocab_size, self.emb_dim, padding_idx=0)\n",
    "        self.embeddings.weight.data.copy_(torch.from_numpy(word_vectors.vectors))\n",
    "        self.embeddings.weight.requires_grad = False\n",
    "\n",
    "        self.linear1 = nn.Linear(self.emb_dim, n_hidden_units)\n",
    "        self.linear2 = nn.Linear(n_hidden_units, n_classes)\n",
    "        self.classifier = nn.Sequential(\n",
    "            self.linear1,\n",
    "            nn.ReLU(),\n",
    "            self.linear2)\n",
    "        self.softmax = nn.Softmax()\n",
    "\n",
    "    def forward(self, input_text, text_len):\n",
    "        \"\"\"\n",
    "        Model forward pass\n",
    "\n",
    "        Keyword arguments:\n",
    "        input_text : vectorized question text\n",
    "        text_len : batch * 1, text length for each question\n",
    "        is_prob: if True, output the softmax of last layer\n",
    "\n",
    "        \"\"\"\n",
    "        # get word embeddings\n",
    "        text_embed = self.embeddings(input_text)\n",
    "\n",
    "        # calculate the mean embeddings\n",
    "        encoded = text_embed.sum(1)\n",
    "        encoded /= text_len.view(text_embed.size(0), -1)\n",
    "\n",
    "        # run data through the classifier\n",
    "        logits = self.classifier(encoded)\n",
    "\n",
    "        return self.softmax(logits)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Question_Dataset(Dataset):\n",
    "    \"\"\"\n",
    "    Pytorch data class for question classfication data\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, examples, lookup):\n",
    "        self.examples = examples\n",
    "        self.word2ind = word2ind\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return vectorize(self.examples[index], self.word2ind)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "\n",
    "def vectorize(ex, word2ind):\n",
    "    \"\"\"\n",
    "    vectorize a single example based on the word2ind dict.\n",
    "\n",
    "    Keyword arguments:\n",
    "    exs: list of input questions-type pairs\n",
    "    ex: tokenized question sentence (list)\n",
    "    label: type of question sentence\n",
    "\n",
    "    Output:  vectorized sentence(python list) and label(int)\n",
    "    e.g. ['text', 'test', 'is', 'fun'] -> [0, 2, 3, 4]\n",
    "    \"\"\"\n",
    "    question_text, question_label = ex\n",
    "    vec_text = [0] * len(question_text)\n",
    "\n",
    "    for idx, token in enumerate(question_text):\n",
    "        if token in word2ind:\n",
    "            vec_text[idx] = word2ind[token]\n",
    "\n",
    "    return vec_text, question_label\n",
    "\n",
    "\n",
    "def batchify(batch):\n",
    "    \"\"\"\n",
    "    Gather a batch of individual examples into one batch,\n",
    "    which includes the question text, question length and labels\n",
    "\n",
    "    Keyword arguments:\n",
    "    batch: list of outputs from vectorize function\n",
    "    \"\"\"\n",
    "\n",
    "    question_len = list()\n",
    "    label_list = list()\n",
    "    for ex in batch:\n",
    "        question_len.append(len(ex[0]))\n",
    "        label_list.append(ans2idx[ex[1]])\n",
    "#     print(label_list)\n",
    "    target_labels = torch.LongTensor(label_list)\n",
    "    x1 = torch.LongTensor(len(question_len), max(question_len)).zero_()\n",
    "    for i in range(len(question_len)):\n",
    "        question_text = batch[i][0]\n",
    "        vec = torch.LongTensor(question_text)\n",
    "        x1[i, :len(question_text)].copy_(vec)\n",
    "    q_batch = {'text': x1, 'len': torch.FloatTensor(question_len), 'labels': target_labels}\n",
    "    return q_batch        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_data_loader, dev_data_loader, accuracy, device):\n",
    "    \"\"\"\n",
    "    Train the current model\n",
    "\n",
    "    Keyword arguments:\n",
    "    model: model to be trained\n",
    "    train_data_loader: pytorch build-in data loader output for training examples\n",
    "    dev_data_loader: pytorch build-in data loader output for dev examples\n",
    "    accuracy: previous best accuracy\n",
    "    device: cpu of gpu\n",
    "    \"\"\"\n",
    "\n",
    "    model.train()\n",
    "    optimizer = torch.optim.Adamax(model.parameters())\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    print_loss_total = 0\n",
    "    epoch_loss_total = 0\n",
    "    start = time.time()\n",
    "\n",
    "    for idx, batch in enumerate(train_data_loader):\n",
    "        question_text = batch['text'].to(device)\n",
    "        question_len = batch['len']\n",
    "        labels = batch['labels']\n",
    "\n",
    "        output = model(question_text, question_len)\n",
    "        loss = criterion(output, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "#         clip_grad_norm_(model.parameters(), grad_clipping)\n",
    "        print_loss_total += loss.data.numpy()\n",
    "        epoch_loss_total += loss.data.numpy()\n",
    "\n",
    "        if idx % checkpoint == 0 and idx > 0:\n",
    "            print_loss_avg = print_loss_total / checkpoint\n",
    "\n",
    "            print('number of steps: %d, loss: %.5f time: %.5f' % (idx, print_loss_avg, time.time()- start))\n",
    "            print_loss_total = 0\n",
    "            curr_accuracy = evaluate(dev_data_loader, model, device)\n",
    "            if accuracy < curr_accuracy:\n",
    "                torch.save(model, save_model)\n",
    "                accuracy = curr_accuracy\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "def evaluate(data_loader, model, device):\n",
    "    \"\"\"\n",
    "    evaluate the current model, get the accuracy for dev/test set\n",
    "\n",
    "    Keyword arguments:\n",
    "    data_loader: pytorch build-in data loader output\n",
    "    model: model to be evaluated\n",
    "    device: cpu of gpu\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    num_examples = 0\n",
    "    error = 0\n",
    "    for idx, batch in enumerate(data_loader):\n",
    "        question_text = batch['text'].to(device)\n",
    "        question_len = batch['len']\n",
    "        labels = batch['labels']\n",
    "\n",
    "        logits = model(question_text, question_len)\n",
    "\n",
    "        top_n, top_i = logits.topk(1)\n",
    "        num_examples += question_text.size(0)\n",
    "        error += torch.nonzero(top_i.squeeze() - torch.LongTensor(labels)).size(0)\n",
    "\n",
    "    accuracy = 1 - error / num_examples\n",
    "    print('accuracy', accuracy)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Train and Dev data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 20\n",
    "\n",
    "# Load batchifed datasets for training (train/dev)\n",
    "train_dataset = Question_Dataset(dev_exs, word2ind)\n",
    "train_sampler = torch.utils.data.sampler.RandomSampler(train_dataset)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size,\n",
    "    sampler=train_sampler, num_workers=0, collate_fn=batchify)\n",
    "\n",
    "\n",
    "dev_dataset = Question_Dataset(dev_exs, word2ind)\n",
    "dev_sampler = torch.utils.data.sampler.SequentialSampler(dev_dataset)\n",
    "dev_loader = torch.utils.data.DataLoader(dev_dataset, batch_size=batch_size,\n",
    "    sampler=dev_sampler, num_workers=0, collate_fn=batchify)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create model instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DanModel(\n",
      "  (embeddings): Embedding(3000000, 300, padding_idx=0)\n",
      "  (linear1): Linear(in_features=300, out_features=50, bias=True)\n",
      "  (linear2): Linear(in_features=50, out_features=26877, bias=True)\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=300, out_features=50, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=50, out_features=26877, bias=True)\n",
      "  )\n",
      "  (softmax): Softmax()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "device = \"cpu\"\n",
    "\n",
    "model = DanModel(len(ans2idx.keys()), n_hidden_units=50)\n",
    "model.to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start epoch 0\n",
      "number of steps: 1, loss: 0.40796 time: 1.59384\n",
      "accuracy 0.0\n",
      "number of steps: 2, loss: 0.20398 time: 3.18918\n",
      "accuracy 0.0\n",
      "number of steps: 3, loss: 0.20398 time: 4.22001\n",
      "accuracy 0.0\n",
      "number of steps: 4, loss: 0.20398 time: 5.22698\n",
      "accuracy 0.0\n",
      "number of steps: 5, loss: 0.20398 time: 6.27542\n",
      "accuracy 0.0\n",
      "number of steps: 6, loss: 0.20398 time: 7.46653\n",
      "accuracy 0.0\n",
      "number of steps: 7, loss: 0.20398 time: 8.46798\n",
      "accuracy 0.0\n",
      "number of steps: 8, loss: 0.20398 time: 9.50597\n",
      "accuracy 0.0\n",
      "number of steps: 9, loss: 0.20398 time: 10.56879\n",
      "accuracy 0.0\n",
      "number of steps: 10, loss: 0.20398 time: 11.65216\n",
      "accuracy 0.0\n",
      "number of steps: 11, loss: 0.20398 time: 12.78137\n",
      "accuracy 0.0\n",
      "number of steps: 12, loss: 0.20398 time: 13.76244\n",
      "accuracy 0.0\n",
      "number of steps: 13, loss: 0.20398 time: 14.73499\n",
      "accuracy 0.0\n",
      "number of steps: 14, loss: 0.20398 time: 15.92432\n",
      "accuracy 0.0\n",
      "number of steps: 15, loss: 0.20398 time: 16.99402\n",
      "accuracy 0.0\n",
      "number of steps: 16, loss: 0.20398 time: 18.15769\n",
      "accuracy 0.0\n",
      "number of steps: 17, loss: 0.20398 time: 19.34721\n",
      "accuracy 0.0004512635379061436\n",
      "number of steps: 18, loss: 0.20398 time: 27.66862\n",
      "accuracy 0.0009025270758122872\n",
      "number of steps: 19, loss: 0.20398 time: 36.34417\n",
      "accuracy 0.0013537906137184308\n",
      "number of steps: 20, loss: 0.20398 time: 45.49665\n",
      "accuracy 0.0013537906137184308\n",
      "number of steps: 21, loss: 0.20398 time: 46.62527\n",
      "accuracy 0.0013537906137184308\n",
      "number of steps: 22, loss: 0.20398 time: 47.81356\n",
      "accuracy 0.0013537906137184308\n",
      "number of steps: 23, loss: 0.20398 time: 48.96356\n",
      "accuracy 0.0018050541516245744\n",
      "number of steps: 24, loss: 0.20398 time: 59.36212\n",
      "accuracy 0.0018050541516245744\n",
      "number of steps: 25, loss: 0.20398 time: 61.99969\n",
      "accuracy 0.0018050541516245744\n",
      "number of steps: 26, loss: 0.20398 time: 63.03029\n",
      "accuracy 0.0013537906137184308\n",
      "number of steps: 27, loss: 0.20398 time: 63.96712\n",
      "accuracy 0.0009025270758122872\n",
      "number of steps: 28, loss: 0.20398 time: 64.98082\n",
      "accuracy 0.0009025270758122872\n",
      "number of steps: 29, loss: 0.20398 time: 66.01224\n",
      "accuracy 0.0004512635379061436\n",
      "number of steps: 30, loss: 0.20398 time: 67.05305\n",
      "accuracy 0.0004512635379061436\n",
      "number of steps: 31, loss: 0.20398 time: 68.16283\n",
      "accuracy 0.0004512635379061436\n",
      "number of steps: 32, loss: 0.20398 time: 69.18487\n",
      "accuracy 0.0004512635379061436\n",
      "number of steps: 33, loss: 0.20398 time: 70.21896\n",
      "accuracy 0.0004512635379061436\n",
      "number of steps: 34, loss: 0.20398 time: 71.25259\n",
      "accuracy 0.0004512635379061436\n",
      "number of steps: 35, loss: 0.20398 time: 72.27681\n",
      "accuracy 0.0004512635379061436\n",
      "number of steps: 36, loss: 0.20398 time: 73.27394\n",
      "accuracy 0.0004512635379061436\n",
      "number of steps: 37, loss: 0.20398 time: 74.36308\n",
      "accuracy 0.0004512635379061436\n",
      "number of steps: 38, loss: 0.20398 time: 75.45625\n",
      "accuracy 0.0004512635379061436\n",
      "number of steps: 39, loss: 0.20398 time: 76.45767\n",
      "accuracy 0.0004512635379061436\n",
      "number of steps: 40, loss: 0.20398 time: 77.47356\n",
      "accuracy 0.0004512635379061436\n",
      "number of steps: 41, loss: 0.20398 time: 78.50337\n",
      "accuracy 0.0004512635379061436\n",
      "number of steps: 42, loss: 0.20398 time: 79.84215\n",
      "accuracy 0.0004512635379061436\n",
      "number of steps: 43, loss: 0.20398 time: 81.09008\n",
      "accuracy 0.0004512635379061436\n",
      "number of steps: 44, loss: 0.20398 time: 82.01238\n",
      "accuracy 0.0004512635379061436\n",
      "number of steps: 45, loss: 0.20398 time: 82.90690\n",
      "accuracy 0.0004512635379061436\n",
      "number of steps: 46, loss: 0.20398 time: 83.79953\n",
      "accuracy 0.0004512635379061436\n",
      "number of steps: 47, loss: 0.20398 time: 84.72735\n",
      "accuracy 0.0004512635379061436\n",
      "number of steps: 48, loss: 0.20398 time: 85.64651\n",
      "accuracy 0.0004512635379061436\n",
      "number of steps: 49, loss: 0.20398 time: 86.47548\n",
      "accuracy 0.0004512635379061436\n",
      "number of steps: 50, loss: 0.20398 time: 87.32403\n",
      "accuracy 0.0004512635379061436\n",
      "number of steps: 51, loss: 0.20398 time: 88.18944\n",
      "accuracy 0.0004512635379061436\n",
      "number of steps: 52, loss: 0.20398 time: 88.96300\n",
      "accuracy 0.0004512635379061436\n",
      "number of steps: 53, loss: 0.20398 time: 89.77927\n",
      "accuracy 0.0009025270758122872\n",
      "number of steps: 54, loss: 0.20398 time: 90.64169\n",
      "accuracy 0.0009025270758122872\n",
      "number of steps: 55, loss: 0.20398 time: 91.61031\n",
      "accuracy 0.0009025270758122872\n",
      "number of steps: 56, loss: 0.20398 time: 92.40664\n",
      "accuracy 0.0009025270758122872\n",
      "number of steps: 57, loss: 0.20398 time: 93.21032\n",
      "accuracy 0.0009025270758122872\n",
      "number of steps: 58, loss: 0.20398 time: 94.02083\n",
      "accuracy 0.0009025270758122872\n",
      "number of steps: 59, loss: 0.20398 time: 94.84808\n",
      "accuracy 0.0009025270758122872\n",
      "number of steps: 60, loss: 0.20398 time: 95.70701\n",
      "accuracy 0.0009025270758122872\n",
      "number of steps: 61, loss: 0.20398 time: 96.51895\n",
      "accuracy 0.0004512635379061436\n",
      "number of steps: 62, loss: 0.20398 time: 97.41703\n",
      "accuracy 0.0004512635379061436\n",
      "number of steps: 63, loss: 0.20398 time: 98.27557\n",
      "accuracy 0.0004512635379061436\n",
      "number of steps: 64, loss: 0.20398 time: 99.12294\n",
      "accuracy 0.0004512635379061436\n",
      "number of steps: 65, loss: 0.20398 time: 99.97280\n",
      "accuracy 0.0004512635379061436\n",
      "number of steps: 66, loss: 0.20398 time: 100.82664\n",
      "accuracy 0.0004512635379061436\n",
      "number of steps: 67, loss: 0.20398 time: 101.78921\n",
      "accuracy 0.0004512635379061436\n",
      "number of steps: 68, loss: 0.20398 time: 102.77876\n",
      "accuracy 0.0004512635379061436\n",
      "number of steps: 69, loss: 0.20398 time: 103.70163\n",
      "accuracy 0.0004512635379061436\n",
      "number of steps: 70, loss: 0.20398 time: 104.68737\n",
      "accuracy 0.0004512635379061436\n",
      "number of steps: 71, loss: 0.20398 time: 105.67008\n",
      "accuracy 0.0004512635379061436\n",
      "number of steps: 72, loss: 0.20398 time: 106.68709\n",
      "accuracy 0.0004512635379061436\n",
      "number of steps: 73, loss: 0.20398 time: 107.73293\n",
      "accuracy 0.0004512635379061436\n",
      "number of steps: 74, loss: 0.20398 time: 108.94600\n",
      "accuracy 0.0004512635379061436\n",
      "number of steps: 75, loss: 0.20398 time: 109.97565\n",
      "accuracy 0.0004512635379061436\n",
      "number of steps: 76, loss: 0.20398 time: 110.94344\n",
      "accuracy 0.0004512635379061436\n",
      "number of steps: 77, loss: 0.20398 time: 111.89664\n",
      "accuracy 0.0004512635379061436\n",
      "number of steps: 78, loss: 0.20398 time: 113.01177\n",
      "accuracy 0.0004512635379061436\n",
      "number of steps: 79, loss: 0.20398 time: 113.94085\n",
      "accuracy 0.0004512635379061436\n",
      "number of steps: 80, loss: 0.20398 time: 115.13156\n",
      "accuracy 0.0004512635379061436\n",
      "number of steps: 81, loss: 0.20398 time: 116.30158\n",
      "accuracy 0.0004512635379061436\n",
      "number of steps: 82, loss: 0.20398 time: 117.41792\n",
      "accuracy 0.0004512635379061436\n",
      "number of steps: 83, loss: 0.20398 time: 118.62319\n",
      "accuracy 0.0004512635379061436\n",
      "number of steps: 84, loss: 0.20398 time: 119.71942\n",
      "accuracy 0.0004512635379061436\n",
      "number of steps: 85, loss: 0.20398 time: 120.73114\n",
      "accuracy 0.0004512635379061436\n",
      "number of steps: 86, loss: 0.20398 time: 121.66400\n",
      "accuracy 0.0004512635379061436\n",
      "number of steps: 87, loss: 0.20398 time: 122.56936\n",
      "accuracy 0.0004512635379061436\n",
      "number of steps: 88, loss: 0.20398 time: 123.59764\n",
      "accuracy 0.0004512635379061436\n",
      "number of steps: 89, loss: 0.20398 time: 124.72048\n",
      "accuracy 0.0004512635379061436\n",
      "number of steps: 90, loss: 0.20398 time: 125.76530\n",
      "accuracy 0.0004512635379061436\n",
      "number of steps: 91, loss: 0.20398 time: 126.78473\n",
      "accuracy 0.0004512635379061436\n",
      "number of steps: 92, loss: 0.20398 time: 127.84113\n",
      "accuracy 0.0004512635379061436\n",
      "number of steps: 93, loss: 0.20398 time: 128.91221\n",
      "accuracy 0.0004512635379061436\n",
      "number of steps: 94, loss: 0.20398 time: 129.93508\n",
      "accuracy 0.0013537906137184308\n",
      "number of steps: 95, loss: 0.20398 time: 130.89390\n",
      "accuracy 0.0013537906137184308\n",
      "number of steps: 96, loss: 0.20398 time: 131.91098\n",
      "accuracy 0.0009025270758122872\n",
      "number of steps: 97, loss: 0.20398 time: 132.98060\n",
      "accuracy 0.0004512635379061436\n",
      "number of steps: 98, loss: 0.20398 time: 134.08398\n",
      "accuracy 0.0009025270758122872\n",
      "number of steps: 99, loss: 0.20398 time: 135.14856\n",
      "accuracy 0.0009025270758122872\n",
      "number of steps: 100, loss: 0.20398 time: 136.19800\n",
      "accuracy 0.0009025270758122872\n",
      "number of steps: 101, loss: 0.20398 time: 137.21640\n",
      "accuracy 0.0013537906137184308\n",
      "number of steps: 102, loss: 0.20398 time: 138.23421\n",
      "accuracy 0.0013537906137184308\n",
      "number of steps: 103, loss: 0.20398 time: 139.25029\n",
      "accuracy 0.0013537906137184308\n",
      "number of steps: 104, loss: 0.20398 time: 140.26957\n",
      "accuracy 0.0013537906137184308\n",
      "number of steps: 105, loss: 0.20398 time: 141.44054\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.0013537906137184308\n",
      "number of steps: 106, loss: 0.20398 time: 142.47461\n",
      "accuracy 0.0013537906137184308\n",
      "number of steps: 107, loss: 0.20398 time: 143.54168\n",
      "accuracy 0.0013537906137184308\n",
      "number of steps: 108, loss: 0.20398 time: 144.45051\n",
      "accuracy 0.0013537906137184308\n",
      "number of steps: 109, loss: 0.20398 time: 145.42052\n",
      "accuracy 0.0013537906137184308\n",
      "number of steps: 110, loss: 0.20398 time: 146.52685\n",
      "accuracy 0.0013537906137184308\n",
      "number of steps: 111, loss: 0.20398 time: 147.56036\n",
      "accuracy 0.0013537906137184308\n",
      "number of steps: 112, loss: 0.20398 time: 148.53932\n",
      "accuracy 0.0013537906137184308\n",
      "number of steps: 113, loss: 0.20398 time: 149.53426\n",
      "accuracy 0.0018050541516245744\n",
      "number of steps: 114, loss: 0.20398 time: 150.62176\n",
      "accuracy 0.0018050541516245744\n",
      "number of steps: 115, loss: 0.20398 time: 151.65729\n",
      "accuracy 0.0018050541516245744\n",
      "number of steps: 116, loss: 0.20398 time: 152.88347\n",
      "accuracy 0.0018050541516245744\n",
      "number of steps: 117, loss: 0.20398 time: 154.11720\n",
      "accuracy 0.0018050541516245744\n",
      "number of steps: 118, loss: 0.20398 time: 155.23931\n",
      "accuracy 0.0018050541516245744\n",
      "number of steps: 119, loss: 0.20398 time: 156.33505\n",
      "accuracy 0.0018050541516245744\n",
      "number of steps: 120, loss: 0.20398 time: 157.52978\n",
      "accuracy 0.0013537906137184308\n",
      "number of steps: 121, loss: 0.20398 time: 158.73195\n",
      "accuracy 0.0013537906137184308\n",
      "number of steps: 122, loss: 0.20398 time: 160.04565\n",
      "accuracy 0.0013537906137184308\n",
      "number of steps: 123, loss: 0.20398 time: 161.34543\n",
      "accuracy 0.0013537906137184308\n",
      "number of steps: 124, loss: 0.20398 time: 162.69115\n",
      "accuracy 0.0013537906137184308\n",
      "number of steps: 125, loss: 0.20398 time: 164.01533\n",
      "accuracy 0.0013537906137184308\n",
      "number of steps: 126, loss: 0.20398 time: 165.26697\n",
      "accuracy 0.0013537906137184308\n",
      "number of steps: 127, loss: 0.20398 time: 166.39660\n",
      "accuracy 0.0013537906137184308\n",
      "number of steps: 128, loss: 0.20398 time: 167.30977\n",
      "accuracy 0.0013537906137184308\n",
      "number of steps: 129, loss: 0.20398 time: 168.28075\n",
      "accuracy 0.0013537906137184308\n",
      "number of steps: 130, loss: 0.20398 time: 169.37528\n",
      "accuracy 0.0013537906137184308\n",
      "number of steps: 131, loss: 0.20398 time: 170.62039\n",
      "accuracy 0.0013537906137184308\n",
      "number of steps: 132, loss: 0.20398 time: 171.79866\n",
      "accuracy 0.0013537906137184308\n",
      "number of steps: 133, loss: 0.20398 time: 173.04727\n",
      "accuracy 0.0013537906137184308\n",
      "number of steps: 134, loss: 0.20398 time: 174.15716\n",
      "accuracy 0.0013537906137184308\n",
      "number of steps: 135, loss: 0.20398 time: 175.20660\n",
      "accuracy 0.0013537906137184308\n",
      "number of steps: 136, loss: 0.20398 time: 176.25434\n",
      "accuracy 0.0013537906137184308\n",
      "number of steps: 137, loss: 0.20398 time: 177.31457\n",
      "accuracy 0.0013537906137184308\n",
      "number of steps: 138, loss: 0.20398 time: 178.30792\n",
      "accuracy 0.0013537906137184308\n",
      "epoch finished in 179 seconds\n",
      "start epoch 1\n",
      "number of steps: 1, loss: 0.40795 time: 0.07342\n",
      "accuracy 0.0013537906137184308\n",
      "number of steps: 2, loss: 0.20397 time: 1.20849\n",
      "accuracy 0.0013537906137184308\n",
      "number of steps: 3, loss: 0.20398 time: 2.46777\n",
      "accuracy 0.0013537906137184308\n",
      "number of steps: 4, loss: 0.20397 time: 3.64945\n",
      "accuracy 0.0013537906137184308\n",
      "number of steps: 5, loss: 0.20397 time: 4.76078\n",
      "accuracy 0.0013537906137184308\n",
      "number of steps: 6, loss: 0.20398 time: 5.89192\n",
      "accuracy 0.0013537906137184308\n",
      "number of steps: 7, loss: 0.20397 time: 6.96182\n",
      "accuracy 0.0013537906137184308\n",
      "number of steps: 8, loss: 0.20397 time: 7.99916\n",
      "accuracy 0.0013537906137184308\n",
      "number of steps: 9, loss: 0.20397 time: 9.03375\n",
      "accuracy 0.0013537906137184308\n",
      "number of steps: 10, loss: 0.20398 time: 10.07445\n",
      "accuracy 0.0013537906137184308\n",
      "number of steps: 11, loss: 0.20397 time: 11.11572\n",
      "accuracy 0.0013537906137184308\n",
      "number of steps: 12, loss: 0.20397 time: 12.17312\n",
      "accuracy 0.0013537906137184308\n",
      "number of steps: 13, loss: 0.20397 time: 13.23431\n",
      "accuracy 0.0013537906137184308\n",
      "number of steps: 14, loss: 0.20397 time: 14.29078\n",
      "accuracy 0.002256317689530718\n",
      "number of steps: 15, loss: 0.20397 time: 32.29875\n",
      "accuracy 0.0027075812274368616\n",
      "number of steps: 16, loss: 0.20397 time: 36.90320\n",
      "accuracy 0.0036101083032491488\n",
      "number of steps: 17, loss: 0.20397 time: 44.90404\n",
      "accuracy 0.002256317689530718\n",
      "number of steps: 18, loss: 0.20397 time: 46.00112\n",
      "accuracy 0.002256317689530718\n",
      "number of steps: 19, loss: 0.20397 time: 47.05882\n",
      "accuracy 0.002256317689530718\n",
      "number of steps: 20, loss: 0.20397 time: 48.10942\n",
      "accuracy 0.002256317689530718\n",
      "number of steps: 21, loss: 0.20397 time: 49.13875\n",
      "accuracy 0.002256317689530718\n",
      "number of steps: 22, loss: 0.20397 time: 50.16401\n",
      "accuracy 0.002256317689530718\n",
      "number of steps: 23, loss: 0.20397 time: 51.15120\n",
      "accuracy 0.002256317689530718\n",
      "number of steps: 24, loss: 0.20397 time: 52.17044\n",
      "accuracy 0.002256317689530718\n",
      "number of steps: 25, loss: 0.20397 time: 53.23565\n",
      "accuracy 0.002256317689530718\n",
      "number of steps: 26, loss: 0.20397 time: 54.43352\n",
      "accuracy 0.002256317689530718\n",
      "number of steps: 27, loss: 0.20397 time: 55.56551\n",
      "accuracy 0.002256317689530718\n",
      "number of steps: 28, loss: 0.20397 time: 56.53791\n",
      "accuracy 0.002256317689530718\n",
      "number of steps: 29, loss: 0.20397 time: 57.70841\n",
      "accuracy 0.002256317689530718\n",
      "number of steps: 30, loss: 0.20396 time: 58.76980\n",
      "accuracy 0.0027075812274368616\n",
      "number of steps: 31, loss: 0.20398 time: 59.85122\n",
      "accuracy 0.002256317689530718\n",
      "number of steps: 32, loss: 0.20397 time: 61.04680\n",
      "accuracy 0.002256317689530718\n",
      "number of steps: 33, loss: 0.20397 time: 62.22899\n",
      "accuracy 0.0018050541516245744\n",
      "number of steps: 34, loss: 0.20397 time: 63.31772\n",
      "accuracy 0.0018050541516245744\n",
      "number of steps: 35, loss: 0.20397 time: 64.48760\n",
      "accuracy 0.0018050541516245744\n",
      "number of steps: 36, loss: 0.20395 time: 65.64037\n",
      "accuracy 0.0018050541516245744\n",
      "number of steps: 37, loss: 0.20398 time: 66.66757\n",
      "accuracy 0.0018050541516245744\n",
      "number of steps: 38, loss: 0.20396 time: 67.68485\n",
      "accuracy 0.0018050541516245744\n",
      "number of steps: 39, loss: 0.20396 time: 68.73035\n",
      "accuracy 0.0018050541516245744\n",
      "number of steps: 40, loss: 0.20397 time: 69.78022\n",
      "accuracy 0.0018050541516245744\n",
      "number of steps: 41, loss: 0.20396 time: 70.83574\n",
      "accuracy 0.0018050541516245744\n",
      "number of steps: 42, loss: 0.20397 time: 71.88873\n",
      "accuracy 0.0018050541516245744\n",
      "number of steps: 43, loss: 0.20397 time: 72.93960\n",
      "accuracy 0.0018050541516245744\n",
      "number of steps: 44, loss: 0.20397 time: 73.97884\n",
      "accuracy 0.0018050541516245744\n",
      "number of steps: 45, loss: 0.20396 time: 75.03122\n",
      "accuracy 0.0018050541516245744\n",
      "number of steps: 46, loss: 0.20398 time: 76.06945\n",
      "accuracy 0.0018050541516245744\n",
      "number of steps: 47, loss: 0.20397 time: 77.09949\n",
      "accuracy 0.0018050541516245744\n",
      "number of steps: 48, loss: 0.20397 time: 78.19293\n",
      "accuracy 0.0018050541516245744\n",
      "number of steps: 49, loss: 0.20396 time: 79.32628\n",
      "accuracy 0.0018050541516245744\n",
      "number of steps: 50, loss: 0.20397 time: 80.33255\n",
      "accuracy 0.0018050541516245744\n",
      "number of steps: 51, loss: 0.20398 time: 81.34523\n",
      "accuracy 0.0018050541516245744\n",
      "number of steps: 52, loss: 0.20397 time: 82.42813\n",
      "accuracy 0.0018050541516245744\n",
      "number of steps: 53, loss: 0.20398 time: 83.49063\n",
      "accuracy 0.0018050541516245744\n",
      "number of steps: 54, loss: 0.20397 time: 84.54036\n",
      "accuracy 0.0018050541516245744\n",
      "number of steps: 55, loss: 0.20397 time: 85.63772\n",
      "accuracy 0.0018050541516245744\n",
      "number of steps: 56, loss: 0.20398 time: 86.80616\n",
      "accuracy 0.0018050541516245744\n",
      "number of steps: 57, loss: 0.20397 time: 87.87866\n",
      "accuracy 0.0018050541516245744\n",
      "number of steps: 58, loss: 0.20398 time: 88.91088\n",
      "accuracy 0.0018050541516245744\n",
      "number of steps: 59, loss: 0.20398 time: 89.96733\n",
      "accuracy 0.0018050541516245744\n",
      "number of steps: 60, loss: 0.20398 time: 91.01103\n",
      "accuracy 0.0018050541516245744\n",
      "number of steps: 61, loss: 0.20398 time: 92.12312\n",
      "accuracy 0.0018050541516245744\n",
      "number of steps: 62, loss: 0.20398 time: 93.42088\n",
      "accuracy 0.0018050541516245744\n",
      "number of steps: 63, loss: 0.20397 time: 94.52869\n",
      "accuracy 0.0018050541516245744\n",
      "number of steps: 64, loss: 0.20395 time: 95.61906\n",
      "accuracy 0.0018050541516245744\n",
      "number of steps: 65, loss: 0.20398 time: 96.71145\n",
      "accuracy 0.0018050541516245744\n",
      "number of steps: 66, loss: 0.20397 time: 97.81745\n",
      "accuracy 0.0018050541516245744\n",
      "number of steps: 67, loss: 0.20397 time: 98.90117\n",
      "accuracy 0.0018050541516245744\n",
      "number of steps: 68, loss: 0.20398 time: 99.99999\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.0018050541516245744\n",
      "number of steps: 69, loss: 0.20398 time: 101.03158\n",
      "accuracy 0.0018050541516245744\n",
      "number of steps: 70, loss: 0.20397 time: 102.07087\n",
      "accuracy 0.0018050541516245744\n",
      "number of steps: 71, loss: 0.20398 time: 103.17307\n",
      "accuracy 0.0018050541516245744\n",
      "number of steps: 72, loss: 0.20395 time: 104.23900\n",
      "accuracy 0.0018050541516245744\n",
      "number of steps: 73, loss: 0.20397 time: 105.28837\n",
      "accuracy 0.0018050541516245744\n",
      "number of steps: 74, loss: 0.20398 time: 106.32892\n",
      "accuracy 0.0018050541516245744\n",
      "number of steps: 75, loss: 0.20397 time: 107.37654\n",
      "accuracy 0.0018050541516245744\n",
      "number of steps: 76, loss: 0.20398 time: 108.42073\n",
      "accuracy 0.0018050541516245744\n",
      "number of steps: 77, loss: 0.20397 time: 109.44545\n",
      "accuracy 0.0018050541516245744\n",
      "number of steps: 78, loss: 0.20398 time: 110.47774\n",
      "accuracy 0.0018050541516245744\n",
      "number of steps: 79, loss: 0.20397 time: 111.51565\n",
      "accuracy 0.0018050541516245744\n",
      "number of steps: 80, loss: 0.20398 time: 112.54607\n",
      "accuracy 0.0018050541516245744\n",
      "number of steps: 81, loss: 0.20397 time: 113.58502\n",
      "accuracy 0.0018050541516245744\n",
      "number of steps: 82, loss: 0.20397 time: 114.62040\n",
      "accuracy 0.0018050541516245744\n",
      "number of steps: 83, loss: 0.20398 time: 115.67468\n",
      "accuracy 0.0018050541516245744\n",
      "number of steps: 84, loss: 0.20391 time: 116.72509\n",
      "accuracy 0.0018050541516245744\n",
      "number of steps: 85, loss: 0.20398 time: 117.77294\n",
      "accuracy 0.0018050541516245744\n",
      "number of steps: 86, loss: 0.20398 time: 118.82416\n",
      "accuracy 0.0018050541516245744\n",
      "number of steps: 87, loss: 0.20398 time: 119.87405\n",
      "accuracy 0.0018050541516245744\n",
      "number of steps: 88, loss: 0.20398 time: 120.91720\n",
      "accuracy 0.0018050541516245744\n",
      "number of steps: 89, loss: 0.20398 time: 121.96111\n",
      "accuracy 0.0018050541516245744\n",
      "number of steps: 90, loss: 0.20398 time: 123.02295\n",
      "accuracy 0.0018050541516245744\n",
      "number of steps: 91, loss: 0.20398 time: 124.09646\n",
      "accuracy 0.0018050541516245744\n",
      "number of steps: 92, loss: 0.20386 time: 125.12633\n",
      "accuracy 0.0018050541516245744\n",
      "number of steps: 93, loss: 0.20398 time: 126.16531\n",
      "accuracy 0.0018050541516245744\n",
      "number of steps: 94, loss: 0.20398 time: 127.20869\n",
      "accuracy 0.0018050541516245744\n",
      "number of steps: 95, loss: 0.20398 time: 128.22642\n",
      "accuracy 0.0018050541516245744\n",
      "number of steps: 96, loss: 0.20396 time: 129.32099\n",
      "accuracy 0.0018050541516245744\n",
      "number of steps: 97, loss: 0.20396 time: 130.46733\n",
      "accuracy 0.0027075812274368616\n",
      "number of steps: 98, loss: 0.20396 time: 131.59050\n",
      "accuracy 0.0036101083032491488\n",
      "number of steps: 99, loss: 0.20398 time: 132.69742\n",
      "accuracy 0.0036101083032491488\n",
      "number of steps: 100, loss: 0.20390 time: 133.81464\n",
      "accuracy 0.004061371841155181\n",
      "number of steps: 101, loss: 0.20398 time: 137.94767\n",
      "accuracy 0.004061371841155181\n",
      "number of steps: 102, loss: 0.20398 time: 139.06489\n",
      "accuracy 0.004061371841155181\n",
      "number of steps: 103, loss: 0.20398 time: 140.15597\n",
      "accuracy 0.004061371841155181\n",
      "number of steps: 104, loss: 0.20398 time: 141.30713\n",
      "accuracy 0.004061371841155181\n",
      "number of steps: 105, loss: 0.20386 time: 142.46693\n",
      "accuracy 0.004061371841155181\n",
      "number of steps: 106, loss: 0.20398 time: 143.73697\n",
      "accuracy 0.0036101083032491488\n",
      "number of steps: 107, loss: 0.20398 time: 145.01914\n",
      "accuracy 0.0018050541516245744\n",
      "number of steps: 108, loss: 0.20397 time: 146.23996\n",
      "accuracy 0.0018050541516245744\n",
      "number of steps: 109, loss: 0.20398 time: 147.25423\n",
      "accuracy 0.0018050541516245744\n",
      "number of steps: 110, loss: 0.20398 time: 148.19824\n",
      "accuracy 0.0018050541516245744\n",
      "number of steps: 111, loss: 0.20398 time: 149.35388\n",
      "accuracy 0.0018050541516245744\n",
      "number of steps: 112, loss: 0.20398 time: 150.59818\n",
      "accuracy 0.0018050541516245744\n",
      "number of steps: 113, loss: 0.20398 time: 151.75487\n",
      "accuracy 0.0018050541516245744\n",
      "number of steps: 114, loss: 0.20398 time: 152.76189\n",
      "accuracy 0.0018050541516245744\n",
      "number of steps: 115, loss: 0.20398 time: 153.96396\n",
      "accuracy 0.0018050541516245744\n",
      "number of steps: 116, loss: 0.20396 time: 155.14735\n",
      "accuracy 0.0018050541516245744\n",
      "number of steps: 117, loss: 0.20396 time: 156.25885\n",
      "accuracy 0.0018050541516245744\n",
      "number of steps: 118, loss: 0.20397 time: 157.35785\n",
      "accuracy 0.0018050541516245744\n",
      "number of steps: 119, loss: 0.20398 time: 158.54263\n",
      "accuracy 0.0018050541516245744\n",
      "number of steps: 120, loss: 0.20398 time: 159.68524\n",
      "accuracy 0.002256317689530718\n",
      "number of steps: 121, loss: 0.20398 time: 160.79962\n",
      "accuracy 0.0036101083032491488\n",
      "number of steps: 122, loss: 0.20398 time: 161.88084\n",
      "accuracy 0.004061371841155181\n",
      "number of steps: 123, loss: 0.20398 time: 162.94251\n",
      "accuracy 0.004061371841155181\n",
      "number of steps: 124, loss: 0.20398 time: 163.95150\n",
      "accuracy 0.004061371841155181\n",
      "number of steps: 125, loss: 0.20398 time: 164.96968\n",
      "accuracy 0.003158844765343005\n",
      "number of steps: 126, loss: 0.20375 time: 166.02377\n",
      "accuracy 0.004061371841155181\n",
      "number of steps: 127, loss: 0.20397 time: 167.13784\n",
      "accuracy 0.004061371841155181\n",
      "number of steps: 128, loss: 0.20398 time: 168.30804\n",
      "accuracy 0.0036101083032491488\n",
      "number of steps: 129, loss: 0.20398 time: 169.57320\n",
      "accuracy 0.003158844765343005\n",
      "number of steps: 130, loss: 0.20398 time: 170.72605\n",
      "accuracy 0.0027075812274368616\n",
      "number of steps: 131, loss: 0.20398 time: 172.00379\n",
      "accuracy 0.0027075812274368616\n",
      "number of steps: 132, loss: 0.20393 time: 173.23372\n",
      "accuracy 0.002256317689530718\n",
      "number of steps: 133, loss: 0.20398 time: 174.52963\n",
      "accuracy 0.002256317689530718\n",
      "number of steps: 134, loss: 0.20398 time: 175.66622\n",
      "accuracy 0.002256317689530718\n",
      "number of steps: 135, loss: 0.20397 time: 176.95877\n",
      "accuracy 0.002256317689530718\n",
      "number of steps: 136, loss: 0.20396 time: 178.09372\n",
      "accuracy 0.002256317689530718\n",
      "number of steps: 137, loss: 0.20398 time: 179.32735\n",
      "accuracy 0.002256317689530718\n",
      "number of steps: 138, loss: 0.20398 time: 180.73230\n",
      "accuracy 0.002256317689530718\n",
      "epoch finished in 182 seconds\n",
      "start epoch 2\n",
      "number of steps: 1, loss: 0.40796 time: 0.12678\n",
      "accuracy 0.002256317689530718\n",
      "number of steps: 2, loss: 0.20398 time: 1.41755\n",
      "accuracy 0.002256317689530718\n",
      "number of steps: 3, loss: 0.20398 time: 2.64585\n",
      "accuracy 0.002256317689530718\n",
      "number of steps: 4, loss: 0.20392 time: 3.70908\n",
      "accuracy 0.002256317689530718\n",
      "number of steps: 5, loss: 0.20398 time: 4.73911\n",
      "accuracy 0.002256317689530718\n",
      "number of steps: 6, loss: 0.20398 time: 5.73246\n",
      "accuracy 0.002256317689530718\n",
      "number of steps: 7, loss: 0.20398 time: 6.74189\n",
      "accuracy 0.002256317689530718\n",
      "number of steps: 8, loss: 0.20397 time: 7.71627\n",
      "accuracy 0.002256317689530718\n",
      "number of steps: 9, loss: 0.20391 time: 8.72543\n",
      "accuracy 0.002256317689530718\n",
      "number of steps: 10, loss: 0.20398 time: 10.00975\n",
      "accuracy 0.002256317689530718\n",
      "number of steps: 11, loss: 0.20397 time: 11.26514\n",
      "accuracy 0.002256317689530718\n",
      "number of steps: 12, loss: 0.20398 time: 12.50893\n",
      "accuracy 0.002256317689530718\n",
      "number of steps: 13, loss: 0.20398 time: 13.78994\n",
      "accuracy 0.002256317689530718\n",
      "number of steps: 14, loss: 0.20398 time: 14.95347\n",
      "accuracy 0.002256317689530718\n",
      "number of steps: 15, loss: 0.20398 time: 16.00513\n",
      "accuracy 0.002256317689530718\n",
      "number of steps: 16, loss: 0.20398 time: 17.02849\n",
      "accuracy 0.002256317689530718\n",
      "number of steps: 17, loss: 0.20314 time: 18.03864\n",
      "accuracy 0.002256317689530718\n",
      "number of steps: 18, loss: 0.20398 time: 19.03236\n",
      "accuracy 0.002256317689530718\n",
      "number of steps: 19, loss: 0.20398 time: 20.01701\n",
      "accuracy 0.002256317689530718\n",
      "number of steps: 20, loss: 0.20397 time: 20.99717\n",
      "accuracy 0.002256317689530718\n",
      "number of steps: 21, loss: 0.20398 time: 21.98008\n",
      "accuracy 0.002256317689530718\n",
      "number of steps: 22, loss: 0.20393 time: 22.94336\n",
      "accuracy 0.002256317689530718\n",
      "number of steps: 23, loss: 0.20397 time: 23.90111\n",
      "accuracy 0.002256317689530718\n",
      "number of steps: 24, loss: 0.20398 time: 25.04255\n",
      "accuracy 0.002256317689530718\n",
      "number of steps: 25, loss: 0.20394 time: 26.33467\n",
      "accuracy 0.002256317689530718\n",
      "number of steps: 26, loss: 0.20308 time: 27.38756\n",
      "accuracy 0.002256317689530718\n",
      "number of steps: 27, loss: 0.20398 time: 28.41445\n",
      "accuracy 0.002256317689530718\n",
      "number of steps: 28, loss: 0.20398 time: 29.48302\n",
      "accuracy 0.002256317689530718\n",
      "number of steps: 29, loss: 0.20398 time: 30.52459\n",
      "accuracy 0.002256317689530718\n",
      "number of steps: 30, loss: 0.20398 time: 31.53303\n",
      "accuracy 0.002256317689530718\n",
      "number of steps: 31, loss: 0.20398 time: 32.53314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.002256317689530718\n",
      "number of steps: 32, loss: 0.20398 time: 33.52992\n",
      "accuracy 0.002256317689530718\n",
      "number of steps: 33, loss: 0.20398 time: 34.54506\n",
      "accuracy 0.002256317689530718\n",
      "number of steps: 34, loss: 0.20398 time: 35.49026\n",
      "accuracy 0.002256317689530718\n",
      "number of steps: 35, loss: 0.20397 time: 36.46550\n",
      "accuracy 0.002256317689530718\n",
      "number of steps: 36, loss: 0.20397 time: 37.43531\n",
      "accuracy 0.002256317689530718\n",
      "number of steps: 37, loss: 0.20398 time: 38.40118\n",
      "accuracy 0.002256317689530718\n",
      "number of steps: 38, loss: 0.20396 time: 39.37026\n",
      "accuracy 0.002256317689530718\n",
      "number of steps: 39, loss: 0.20398 time: 40.34175\n",
      "accuracy 0.002256317689530718\n",
      "number of steps: 40, loss: 0.20397 time: 41.48265\n",
      "accuracy 0.002256317689530718\n",
      "number of steps: 41, loss: 0.20398 time: 42.50779\n",
      "accuracy 0.002256317689530718\n",
      "number of steps: 42, loss: 0.20398 time: 43.48731\n",
      "accuracy 0.002256317689530718\n",
      "number of steps: 43, loss: 0.20398 time: 44.58255\n",
      "accuracy 0.002256317689530718\n",
      "number of steps: 44, loss: 0.20398 time: 45.65489\n",
      "accuracy 0.002256317689530718\n",
      "number of steps: 45, loss: 0.20291 time: 46.73513\n",
      "accuracy 0.002256317689530718\n",
      "number of steps: 46, loss: 0.20398 time: 47.66546\n",
      "accuracy 0.002256317689530718\n",
      "number of steps: 47, loss: 0.20398 time: 48.53740\n",
      "accuracy 0.002256317689530718\n",
      "number of steps: 48, loss: 0.20397 time: 49.46246\n",
      "accuracy 0.002256317689530718\n",
      "number of steps: 49, loss: 0.20398 time: 50.45807\n",
      "accuracy 0.002256317689530718\n",
      "number of steps: 50, loss: 0.20398 time: 51.44065\n",
      "accuracy 0.002256317689530718\n",
      "number of steps: 51, loss: 0.20398 time: 52.52961\n",
      "accuracy 0.002256317689530718\n",
      "number of steps: 52, loss: 0.20398 time: 53.76223\n",
      "accuracy 0.002256317689530718\n",
      "number of steps: 53, loss: 0.20398 time: 54.95542\n",
      "accuracy 0.002256317689530718\n",
      "number of steps: 54, loss: 0.20398 time: 56.19462\n",
      "accuracy 0.002256317689530718\n",
      "number of steps: 55, loss: 0.20398 time: 57.34284\n",
      "accuracy 0.002256317689530718\n",
      "number of steps: 56, loss: 0.20278 time: 58.40952\n",
      "accuracy 0.002256317689530718\n",
      "number of steps: 57, loss: 0.20398 time: 59.44727\n",
      "accuracy 0.002256317689530718\n",
      "number of steps: 58, loss: 0.20398 time: 60.49045\n",
      "accuracy 0.002256317689530718\n",
      "number of steps: 59, loss: 0.20398 time: 61.53723\n",
      "accuracy 0.002256317689530718\n",
      "number of steps: 60, loss: 0.20398 time: 62.56275\n",
      "accuracy 0.002256317689530718\n",
      "number of steps: 61, loss: 0.20398 time: 63.60196\n",
      "accuracy 0.002256317689530718\n",
      "number of steps: 62, loss: 0.20398 time: 64.62379\n",
      "accuracy 0.002256317689530718\n",
      "number of steps: 63, loss: 0.20398 time: 65.65905\n",
      "accuracy 0.002256317689530718\n",
      "number of steps: 64, loss: 0.20398 time: 66.67265\n",
      "accuracy 0.002256317689530718\n",
      "number of steps: 65, loss: 0.20398 time: 67.65552\n",
      "accuracy 0.002256317689530718\n",
      "number of steps: 66, loss: 0.20398 time: 68.55102\n",
      "accuracy 0.002256317689530718\n",
      "number of steps: 67, loss: 0.20398 time: 69.45417\n",
      "accuracy 0.002256317689530718\n",
      "number of steps: 68, loss: 0.20398 time: 70.44249\n",
      "accuracy 0.002256317689530718\n",
      "number of steps: 69, loss: 0.20398 time: 71.43026\n",
      "accuracy 0.002256317689530718\n",
      "number of steps: 70, loss: 0.20398 time: 72.47577\n",
      "accuracy 0.002256317689530718\n",
      "number of steps: 71, loss: 0.20398 time: 73.46655\n",
      "accuracy 0.002256317689530718\n",
      "number of steps: 72, loss: 0.20398 time: 74.53329\n",
      "accuracy 0.002256317689530718\n",
      "number of steps: 73, loss: 0.20398 time: 75.57966\n",
      "accuracy 0.002256317689530718\n",
      "number of steps: 74, loss: 0.20398 time: 76.56132\n",
      "accuracy 0.002256317689530718\n",
      "number of steps: 75, loss: 0.20398 time: 77.54637\n",
      "accuracy 0.002256317689530718\n",
      "number of steps: 76, loss: 0.20398 time: 78.52655\n",
      "accuracy 0.002256317689530718\n",
      "number of steps: 77, loss: 0.20398 time: 79.60886\n",
      "accuracy 0.002256317689530718\n",
      "number of steps: 78, loss: 0.20274 time: 80.76861\n",
      "accuracy 0.002256317689530718\n",
      "number of steps: 79, loss: 0.20398 time: 81.88716\n",
      "accuracy 0.002256317689530718\n",
      "number of steps: 80, loss: 0.20398 time: 83.00933\n",
      "accuracy 0.002256317689530718\n",
      "number of steps: 81, loss: 0.20398 time: 84.09377\n",
      "accuracy 0.002256317689530718\n",
      "number of steps: 82, loss: 0.20398 time: 85.37165\n",
      "accuracy 0.002256317689530718\n",
      "number of steps: 83, loss: 0.20398 time: 86.52722\n",
      "accuracy 0.002256317689530718\n",
      "number of steps: 84, loss: 0.20398 time: 87.73242\n",
      "accuracy 0.002256317689530718\n",
      "number of steps: 85, loss: 0.20398 time: 89.06854\n",
      "accuracy 0.002256317689530718\n",
      "number of steps: 86, loss: 0.20398 time: 90.32577\n",
      "accuracy 0.002256317689530718\n",
      "number of steps: 87, loss: 0.20398 time: 91.57120\n",
      "accuracy 0.002256317689530718\n",
      "number of steps: 88, loss: 0.20398 time: 92.79999\n",
      "accuracy 0.002256317689530718\n",
      "number of steps: 89, loss: 0.20398 time: 93.90656\n",
      "accuracy 0.002256317689530718\n",
      "number of steps: 90, loss: 0.20398 time: 94.95031\n",
      "accuracy 0.002256317689530718\n",
      "number of steps: 91, loss: 0.20398 time: 95.98160\n",
      "accuracy 0.002256317689530718\n",
      "number of steps: 92, loss: 0.20398 time: 97.00846\n",
      "accuracy 0.002256317689530718\n",
      "number of steps: 93, loss: 0.20398 time: 98.02504\n",
      "accuracy 0.002256317689530718\n",
      "number of steps: 94, loss: 0.20398 time: 99.06420\n",
      "accuracy 0.002256317689530718\n",
      "number of steps: 95, loss: 0.20398 time: 100.18402\n",
      "accuracy 0.002256317689530718\n",
      "number of steps: 96, loss: 0.20398 time: 101.41437\n",
      "accuracy 0.002256317689530718\n",
      "number of steps: 97, loss: 0.20398 time: 102.65060\n",
      "accuracy 0.002256317689530718\n",
      "number of steps: 98, loss: 0.20398 time: 103.80653\n",
      "accuracy 0.002256317689530718\n",
      "number of steps: 99, loss: 0.20398 time: 104.85538\n",
      "accuracy 0.002256317689530718\n",
      "number of steps: 100, loss: 0.20398 time: 105.87368\n",
      "accuracy 0.002256317689530718\n",
      "number of steps: 101, loss: 0.20398 time: 106.89630\n",
      "accuracy 0.002256317689530718\n",
      "number of steps: 102, loss: 0.20398 time: 108.11443\n",
      "accuracy 0.002256317689530718\n",
      "number of steps: 103, loss: 0.20398 time: 109.22238\n",
      "accuracy 0.002256317689530718\n",
      "number of steps: 104, loss: 0.20398 time: 110.26037\n",
      "accuracy 0.002256317689530718\n",
      "number of steps: 105, loss: 0.20398 time: 111.21615\n",
      "accuracy 0.002256317689530718\n",
      "number of steps: 106, loss: 0.20398 time: 112.23346\n",
      "accuracy 0.002256317689530718\n",
      "number of steps: 107, loss: 0.20398 time: 113.25477\n",
      "accuracy 0.002256317689530718\n",
      "number of steps: 108, loss: 0.20398 time: 114.34576\n",
      "accuracy 0.002256317689530718\n",
      "number of steps: 109, loss: 0.20398 time: 115.49507\n",
      "accuracy 0.002256317689530718\n",
      "number of steps: 110, loss: 0.20398 time: 116.67285\n",
      "accuracy 0.002256317689530718\n",
      "number of steps: 111, loss: 0.20398 time: 117.76516\n",
      "accuracy 0.002256317689530718\n",
      "number of steps: 112, loss: 0.20398 time: 118.80122\n",
      "accuracy 0.002256317689530718\n",
      "number of steps: 113, loss: 0.20398 time: 119.78914\n",
      "accuracy 0.002256317689530718\n",
      "number of steps: 114, loss: 0.20398 time: 120.78901\n",
      "accuracy 0.002256317689530718\n",
      "number of steps: 115, loss: 0.20398 time: 121.78264\n",
      "accuracy 0.002256317689530718\n",
      "number of steps: 116, loss: 0.20398 time: 122.83047\n",
      "accuracy 0.002256317689530718\n",
      "number of steps: 117, loss: 0.20398 time: 123.90281\n",
      "accuracy 0.002256317689530718\n",
      "number of steps: 118, loss: 0.20398 time: 125.04511\n",
      "accuracy 0.002256317689530718\n",
      "number of steps: 119, loss: 0.20398 time: 126.22611\n",
      "accuracy 0.002256317689530718\n",
      "number of steps: 120, loss: 0.20398 time: 127.45287\n",
      "accuracy 0.002256317689530718\n",
      "number of steps: 121, loss: 0.20398 time: 128.48558\n",
      "accuracy 0.002256317689530718\n",
      "number of steps: 122, loss: 0.20398 time: 129.38637\n",
      "accuracy 0.002256317689530718\n",
      "number of steps: 123, loss: 0.20398 time: 130.30334\n",
      "accuracy 0.002256317689530718\n",
      "number of steps: 124, loss: 0.20398 time: 131.45238\n",
      "accuracy 0.002256317689530718\n",
      "number of steps: 125, loss: 0.20398 time: 132.68787\n",
      "accuracy 0.002256317689530718\n",
      "number of steps: 126, loss: 0.20398 time: 133.93929\n",
      "accuracy 0.002256317689530718\n",
      "number of steps: 127, loss: 0.20398 time: 135.11029\n",
      "accuracy 0.002256317689530718\n",
      "number of steps: 128, loss: 0.20398 time: 136.18735\n",
      "accuracy 0.002256317689530718\n",
      "number of steps: 129, loss: 0.20398 time: 137.31160\n",
      "accuracy 0.002256317689530718\n",
      "number of steps: 130, loss: 0.20398 time: 138.39208\n",
      "accuracy 0.002256317689530718\n",
      "number of steps: 131, loss: 0.20398 time: 139.40291\n",
      "accuracy 0.002256317689530718\n",
      "number of steps: 132, loss: 0.20398 time: 140.44263\n",
      "accuracy 0.002256317689530718\n",
      "number of steps: 133, loss: 0.20398 time: 141.47989\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.002256317689530718\n",
      "number of steps: 134, loss: 0.20398 time: 142.51975\n",
      "accuracy 0.002256317689530718\n",
      "number of steps: 135, loss: 0.20398 time: 143.67011\n",
      "accuracy 0.002256317689530718\n",
      "number of steps: 136, loss: 0.20398 time: 144.83827\n",
      "accuracy 0.002256317689530718\n",
      "number of steps: 137, loss: 0.20398 time: 145.95305\n",
      "accuracy 0.002256317689530718\n",
      "number of steps: 138, loss: 0.20398 time: 147.11266\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-796a61fc2bf6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'start epoch %d'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"epoch finished in {} seconds\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-37-87999798e35f>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_data_loader, dev_data_loader, accuracy, device)\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'number of steps: %d, loss: %.5f time: %.5f'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_loss_avg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0mprint_loss_total\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m             \u001b[0mcurr_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdev_data_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mcurr_accuracy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m                 \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-37-87999798e35f>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(data_loader, model, device)\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquestion_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0mtop_n\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_i\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtopk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m         \u001b[0mnum_examples\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mquestion_text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0merror\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop_i\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Start Training\n",
    "checkpoint = 50\n",
    "num_epochs = 20\n",
    "accuracy = 0\n",
    "\n",
    "# Create testing dataloader\n",
    "test_dataset = Question_Dataset(test_exs, word2ind)\n",
    "test_sampler = torch.utils.data.sampler.SequentialSampler(test_dataset)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size,\n",
    "    sampler=test_sampler, num_workers=0, collate_fn=batchify)\n",
    "\n",
    "# Train / Fit\n",
    "for epoch in range(num_epochs):\n",
    "    start = time.time()\n",
    "    print('start epoch %d' % epoch)\n",
    "    accuracy = train(model, train_loader, dev_loader, accuracy, device)\n",
    "    print(\"epoch finished in {} seconds\".format(int(time.time() - start)))\n",
    "    \n",
    "# Test\n",
    "print('\\nstart testing:\\n')\n",
    "evaluate(test_loader, model, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cmsc723",
   "language": "python",
   "name": "cmsc723"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
