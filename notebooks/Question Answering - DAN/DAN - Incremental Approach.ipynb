{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DAN for Question Answering\n",
    "\n",
    "Notebook to take a basic DAN for question answering and attempt to training on larger and larger datasets.  We start with 10 questions/answers which gets 85% accuracy.  We can then attempt to modify the number of hidden nodes, etc. in order to maintain accuracy as we enlarge the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook \n",
    "\n",
    "import time\n",
    "import json\n",
    "import random\n",
    "from pprint import pprint\n",
    "from collections import namedtuple\n",
    "from nltk.probability import FreqDist\n",
    "from pprint import pprint\n",
    "from nltk import word_tokenize\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "from gensim.models import word2vec\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "Run = namedtuple(\"Run\", \"epochs n_hidden_units training_size elapsed accuracy batch_size accuracies\")\n",
    "RUNS = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../../data/GoogleNews-vectors-negative300.bin\"\n",
    "word_vectors = KeyedVectors.load_word2vec_format(path, binary=True)\n",
    "word2ind = {k: v.index for k,v in word_vectors.vocab.items()}\n",
    "ind2word = {v:k for k,v in word2ind.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = \"../../data/qanta.train.2018.04.18.json\"\n",
    "dev_file = \"../../data/qanta.dev.2018.04.18.json\"\n",
    "test_file = \"../../data/qanta.test.2018.04.18.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filename):\n",
    "    data = list()\n",
    "    with open(filename) as json_data:\n",
    "        questions = json.load(json_data)[\"questions\"]\n",
    "        \n",
    "#         for q in questions:\n",
    "#             q_text = q['text'].split()\n",
    "#             label = category_lookup[q['category']]\n",
    "#             data.append((q_text, label))\n",
    "    return questions\n",
    "\n",
    "\n",
    "train_data = load_data(train_file)\n",
    "dev_data = load_data(dev_file) \n",
    "test_data = load_data(test_file)\n",
    "all_data = train_data + dev_data + test_data\n",
    "\n",
    "# all_data = all_data[:100]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create lookups for answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model = \"qa-dan.pt\"\n",
    "grad_clipping = 5\n",
    "checkpoint = 5\n",
    "\n",
    "class DanModel(nn.Module):\n",
    "\n",
    "    def __init__(self, n_classes, n_hidden_units):\n",
    "        super(DanModel, self).__init__()\n",
    "        self.n_classes = n_classes\n",
    "        self.n_hidden_units = n_hidden_units\n",
    "        \n",
    "        self.vocab_size, self.emb_dim = word_vectors.vectors.shape\n",
    "        self.embeddings = nn.Embedding(self.vocab_size, self.emb_dim, padding_idx=0)\n",
    "        self.embeddings.weight.data.copy_(torch.from_numpy(word_vectors.vectors))\n",
    "        self.embeddings.weight.requires_grad = False\n",
    "\n",
    "#         self.linear1 = nn.Linear(self.emb_dim, n_hidden_units)\n",
    "#         self.linear2 = nn.Linear(n_hidden_units, 50)\n",
    "#         self.linear3 = nn.Linear(50, n_classes)\n",
    "#         self.classifier = nn.Sequential(\n",
    "#             self.linear1,\n",
    "#             nn.ReLU(),\n",
    "#             self.linear2,\n",
    "#             nn.ReLU(),\n",
    "#             self.linear3)\n",
    "\n",
    "\n",
    "        self.linear1 = nn.Linear(self.emb_dim, n_hidden_units)\n",
    "        self.linear2 = nn.Linear(n_hidden_units, n_classes)\n",
    "        self.classifier = nn.Sequential(\n",
    "            self.linear1,\n",
    "            nn.ReLU(),\n",
    "            self.linear2)\n",
    "        self.softmax = nn.Softmax()\n",
    "\n",
    "    def forward(self, input_text, text_len):\n",
    "        \"\"\"\n",
    "        Model forward pass\n",
    "\n",
    "        Keyword arguments:\n",
    "        input_text : vectorized question text\n",
    "        text_len : batch * 1, text length for each question\n",
    "        is_prob: if True, output the softmax of last layer\n",
    "\n",
    "        \"\"\"\n",
    "        # get word embeddings\n",
    "        text_embed = self.embeddings(input_text)\n",
    "\n",
    "        # calculate the mean embeddings\n",
    "        encoded = text_embed.sum(1)\n",
    "        encoded /= text_len.view(text_embed.size(0), -1)\n",
    "\n",
    "        # run data through the classifier\n",
    "        logits = self.classifier(encoded)\n",
    "\n",
    "        return self.softmax(logits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Question_Dataset(Dataset):\n",
    "\n",
    "    def __init__(self, examples, lookup):\n",
    "        self.examples = examples\n",
    "        self.lookup = lookup\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return vectorize(self.examples[index], self.lookup)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "\n",
    "def vectorize(ex, lookup):\n",
    "    \"\"\"\n",
    "    vectorize a single example based on the word2ind dict.\n",
    "\n",
    "    Keyword arguments:\n",
    "    exs: list of input questions-type pairs\n",
    "    ex: tokenized question sentence (list)\n",
    "    label: type of question sentence\n",
    "\n",
    "    Output:  vectorized sentence(python list) and label(int)\n",
    "    e.g. ['text', 'test', 'is', 'fun'] -> [0, 2, 3, 4]\n",
    "    \"\"\"\n",
    "    vec_text = []\n",
    "    question_text, question_label = ex\n",
    "    \n",
    "    for idx, token in enumerate(question_text):\n",
    "        if token in lookup:\n",
    "            vec_text.append(lookup[token])\n",
    "\n",
    "    return vec_text, question_label\n",
    "\n",
    "\n",
    "def batchify(batch):\n",
    "    \"\"\"\n",
    "    Gather a batch of individual examples into one batch,\n",
    "    which includes the question text, question length and labels\n",
    "\n",
    "    Keyword arguments:\n",
    "    batch: list of outputs from vectorize function\n",
    "    \"\"\"\n",
    "\n",
    "    question_len = list()\n",
    "    label_list = list()\n",
    "    for ex in batch:\n",
    "        question_len.append(len(ex[0]))\n",
    "        label_list.append(ex[1])\n",
    "    target_labels = torch.LongTensor(label_list)\n",
    "    x1 = torch.LongTensor(len(question_len), max(question_len)).zero_()\n",
    "    for i in range(len(question_len)):\n",
    "        question_text = batch[i][0]\n",
    "        vec = torch.LongTensor(question_text)\n",
    "        x1[i, :len(question_text)].copy_(vec)\n",
    "    q_batch = {'text': x1, 'len': torch.FloatTensor(question_len), 'labels': target_labels}\n",
    "    return q_batch        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_data_loader, dev_data_loader, accuracy, device):\n",
    "    \"\"\"\n",
    "    Train the current model\n",
    "\n",
    "    Keyword arguments:\n",
    "    model: model to be trained\n",
    "    train_data_loader: pytorch build-in data loader output for training examples\n",
    "    dev_data_loader: pytorch build-in data loader output for dev examples\n",
    "    accuracy: previous best accuracy\n",
    "    device: cpu of gpu\n",
    "    \"\"\"\n",
    "\n",
    "    model.train()\n",
    "    optimizer = torch.optim.Adamax(model.parameters())\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    print_loss_total = 0\n",
    "    curr_accuracy = 0\n",
    "    start = time.time()\n",
    "\n",
    "    for idx, batch in enumerate(train_data_loader):\n",
    "        question_text = batch['text'].to(device)\n",
    "        question_len = batch['len']\n",
    "        labels = batch['labels']\n",
    "\n",
    "        output = model(question_text, question_len)\n",
    "        loss = criterion(output, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "#         clip_grad_norm_(model.parameters(), grad_clipping)\n",
    "#         print_loss_total += loss.data.numpy()\n",
    "\n",
    "#         if idx % checkpoint == 0 and idx > 0:\n",
    "#             print_loss_avg = print_loss_total / checkpoint\n",
    "\n",
    "#             print('number of steps: %d, loss: %.5f time: %.5f' % (idx, print_loss_avg, time.time()- start))\n",
    "#             print_loss_total = 0\n",
    "#             curr_accuracy = 0\n",
    "# #             curr_accuracy = evaluate(dev_data_loader, model, device)\n",
    "#             if accuracy < curr_accuracy:\n",
    "#                 torch.save(model, save_model)\n",
    "#                 accuracy = curr_accuracy\n",
    "                \n",
    "    return curr_accuracy\n",
    "\n",
    "\n",
    "def evaluate(data_loader, model, device):\n",
    "    \"\"\"\n",
    "    evaluate the current model, get the accuracy for dev/test set\n",
    "\n",
    "    Keyword arguments:\n",
    "    data_loader: pytorch build-in data loader output\n",
    "    model: model to be evaluated\n",
    "    device: cpu of gpu\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    num_examples = 0\n",
    "    error = 0\n",
    "    for idx, batch in enumerate(data_loader):\n",
    "        question_text = batch['text'].to(device)\n",
    "        question_len = batch['len']\n",
    "        labels = batch['labels']\n",
    "\n",
    "        logits = model(question_text, question_len)\n",
    "\n",
    "        top_n, top_i = logits.topk(1)\n",
    "        num_examples += question_text.size(0)\n",
    "        error += torch.nonzero(top_i.squeeze() - torch.LongTensor(labels)).size(0)\n",
    "\n",
    "    accuracy = 1 - error / num_examples\n",
    "#     print('accuracy', accuracy)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import trange, tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DanModel(\n",
      "  (embeddings): Embedding(3000000, 300, padding_idx=0)\n",
      "  (linear1): Linear(in_features=300, out_features=2000, bias=True)\n",
      "  (linear2): Linear(in_features=2000, out_features=26878, bias=True)\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=300, out_features=2000, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=2000, out_features=26878, bias=True)\n",
      "  )\n",
      "  (softmax): Softmax()\n",
      ")\n",
      "Setup finished in 133 seconds\n",
      "\n",
      "dev_set is of length 11924\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c83e74715f94476a5a4b45f7b346b2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\r",
      "\r",
      "Epoch: 0, Accuracy: 0.0007547802750754729\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-62-8f727540c3e9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;31m#     print('start epoch %d' % epoch)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m     \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdev_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0maccuracies\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-50-79e558a19e2c>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_data_loader, dev_data_loader, accuracy, device)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/cmsc723/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/cmsc723/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Start Training\n",
    "device = \"cpu\"\n",
    "n_hidden_units = 2000\n",
    "checkpoint = 2\n",
    "batch_size = 512\n",
    "num_epochs = 1000\n",
    "start = time.time()\n",
    "\n",
    "accuracy = 0\n",
    "accuracies = []\n",
    "\n",
    "data = all_data#[:2000]\n",
    "# ans2idx = {q[\"page\"]: idx for idx, q in enumerate(data) if q[\"page\"]}\n",
    "ans2idx = {}\n",
    "\n",
    "counter = 0\n",
    "for idx, q in enumerate(data):\n",
    "    if q[\"page\"] not in ans2idx:\n",
    "        ans2idx[q[\"page\"]] = counter\n",
    "        counter += 1\n",
    "        \n",
    "\n",
    "data = [(word_tokenize(q[\"text\"]), ans2idx[q[\"page\"]]) for q in data]\n",
    "\n",
    "n_classes = len(ans2idx.keys()) + 1\n",
    "\n",
    "model = DanModel(n_classes, n_hidden_units=n_hidden_units)\n",
    "model.to(device)\n",
    "print(model)\n",
    "print(\"Setup finished in {} seconds\\n\".format(int(time.time() - start)))\n",
    "\n",
    "# Create testing dataloader\n",
    "train_dataset = Question_Dataset(data, word2ind)\n",
    "train_sampler = torch.utils.data.sampler.RandomSampler(train_dataset)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size,\n",
    "    sampler=train_sampler, num_workers=0, collate_fn=batchify)\n",
    "\n",
    "dev_size = int(len(data) / 10)\n",
    "dev_data = data[:dev_size] # random.sample(data, dev_size)\n",
    "print(f\"dev_set is of length {dev_size}\")\n",
    "dev_dataset = Question_Dataset(dev_data, word2ind)\n",
    "dev_sampler = torch.utils.data.sampler.SequentialSampler(dev_dataset)\n",
    "dev_loader = torch.utils.data.DataLoader(dev_dataset, batch_size=batch_size,\n",
    "    sampler=dev_sampler, num_workers=0, collate_fn=batchify)\n",
    "\n",
    "\n",
    "# Train / Fit\n",
    "start = time.time()\n",
    "iterable = tqdm_notebook(range(num_epochs))\n",
    "for epoch in iterable:\n",
    "#     print('start epoch %d' % epoch)\n",
    "    train(model, train_loader, dev_loader, accuracy, device)\n",
    "    accuracy = evaluate(dev_loader, model, device)\n",
    "    accuracies.append(accuracy)\n",
    "    if epoch % 100 == 0:\n",
    "        iterable.write(\"Epoch: {}, Accuracy: {}\".format(epoch, accuracy))\n",
    "\n",
    "elapsed = int(time.time() - start)\n",
    "print(\"Training complete in {} seconds\\n\".format(elapsed))\n",
    "\n",
    "# Test\n",
    "print('\\nstart testing:\\n')\n",
    "accuracy = evaluate(train_loader, model, device)\n",
    "print(\"Final accuracy on training set: {}\".format(accuracy))\n",
    "\n",
    "RUNS.append(Run(\n",
    "    epochs=num_epochs, \n",
    "    n_hidden_units=n_hidden_units, \n",
    "    training_size=len(data),\n",
    "    elapsed=elapsed,\n",
    "    accuracy=accuracy,\n",
    "    accuracies=accuracies,\n",
    "    batch_size=batch_size\n",
    "))\n",
    "\n",
    "_ = pd.Series(accuracies).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cmsc723",
   "language": "python",
   "name": "cmsc723"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
